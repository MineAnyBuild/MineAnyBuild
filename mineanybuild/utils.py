import json
import os
from ast import literal_eval
import cv2
import shutil
import re
from typing import Any

####################################################################
## JSON Parser
####################################################################


def is_3d_list(obj: Any) -> bool:
    if not isinstance(obj, list):
        return False
    
    for dim2 in obj:
        if not isinstance(dim2, list):
            return False
        for dim3 in dim2:
            if not isinstance(dim3, list):
                return False
            for elem in dim3:
                if isinstance(elem, list):
                    return False
    return True


def json_parser_blueprint(input_root, error_file="processing_errors.json", proprietary_models = ['claude-3-5-sonnet', 'claude-3-7-sonnet-latest', 'gemini-1.5-flash-latest', 'gemini-1.5-pro', 'gemini-2.0-flash', 'gpt-4o', 'gpt-4o-mini']):
    '''
    Parse the output of MLLM-based agents generated by prompter.py into usable 'blueprint' data.
    
    '''
    tasks = os.listdir(input_root)
    task_out = {"valid": 0, "error": 0}
    
    for task in tasks:
        error_log = []
        valid_num = 0
        task_root = os.path.join(input_root, task)
        error_file = os.path.join(task_root, 'errors.json')
        response_dir = os.path.join(task_root, 'llm_response')
        output_dir = os.path.join(task_root, 'llm_response_blueprint')
        
        if not os.path.exists(response_dir):
            error_log.append({
                "type": "missing_directory",
                "path": response_dir,
                "task": task
            })
            continue
            
        os.makedirs(output_dir, exist_ok=True)
        
        for arch in os.listdir(response_dir):
            arch_dir = os.path.join(response_dir, arch)
            output_path = os.path.join(output_dir, f"{arch}.json")
            valid_data = {}
            
            for pm in proprietary_models:
                json_path = os.path.join(arch_dir, f"{pm}.json")
                try:
                    with open(json_path, 'r') as f1:
                        raw_string = json.load(f1)
                    json_match = re.search(
                        r'(?s)(?:```|\'\'\')json\n(.*?)(?:```|\'\'\')',
                        raw_string
                    )
                    if not json_match:
                        raise ValueError("No valid JSON code block found")
                    dirty_json = json_match.group(1)
                    clean_json = re.sub(
                        r'\s*#.*?$|//.*?$', 
                        '', 
                        dirty_json.replace('\u2010', '-').replace('\u2212', '-'),
                        flags=re.MULTILINE
                    ).strip()
                    processed = re.sub(r',\s*([}\]])', r'\1', clean_json)
                    compact_json = re.sub(r'\s+', '', processed)
                    
                    data = json.loads(compact_json)
                    if not is_3d_list(data):
                        raise ValueError("Data structure is not a valid 3D list")
                    
                    valid_data[pm] = data
                    
                except Exception as e:
                    error_log.append({
                        "task": task,
                        "architecture": arch,
                        "model": pm,
                        "error_type": type(e).__name__,
                        "error_msg": str(e),
                        "file_path": json_path,
                        "validation_failed": isinstance(e, ValueError) and "3D" in str(e)
                    })
                    continue
                    
            if valid_data:
                valid_num += len(valid_data)
                with open(output_path, 'w') as f:
                    json.dump(valid_data, f, indent=4, ensure_ascii=False)


        task_out["valid"] += valid_num
        task_out["error"] += len(error_log)

        with open(error_file, 'w') as f:
            json.dump(error_log, f, indent=4, ensure_ascii=False)


def process_json(input_dir, output_file):
    print(input_dir.split('/')[1])
    consolidated_data = {}
    archs = os.listdir(input_dir)
    for arch in archs:
        arch_name = arch.split('.json')[0]
        print(arch_name)
        filename = os.path.join(input_dir, arch)
        out = {}
        with open(filename, 'r') as f:
            data = json.load(f)
        for k, v in data.items():
            data_dict = literal_eval(v)
            # data_dict = json.loads(v)
            out[k] = data_dict
        consolidated_data[arch_name] = out
            
    with open(output_file, 'w') as f:
        json.dump(consolidated_data, f, indent=2, ensure_ascii=False)



def process_json_critic_scores(input_dir, output_file, error_file):
    # critic score
    consolidated_data = {}
    error_files = []

    archs = os.listdir(input_dir)
    for arch in archs:
        arch_path = os.path.join(input_dir, arch)
        arch_name = os.path.splitext(arch)[0]
        
        try:
            with open(arch_path, 'r') as f:
                data = json.load(f)
            
            out = {}
            for k, v in data.items():
                try:
                    data_dict = literal_eval(v.strip())
                    out[k] = data_dict
                except Exception as e:
                    raise ValueError(f"Value parse error at key '{k}': {str(e)}")
            
            consolidated_data[arch_name] = out
            
        except Exception as e:
            print(f"Error processing {arch}: {str(e)}")
            error_files.append({
                "file_path": arch_path,
                "error": str(e)
            })
    
    with open(output_file, 'w') as f:
        json.dump(consolidated_data, f, indent=2, ensure_ascii=False)
    
    with open(error_file, 'w') as f:
        json.dump(error_files, f, indent=2, ensure_ascii=False)



####################################################################
## Video Clipper
####################################################################

def frame_clipper(videos_src_path, videos_save_path):
    each_video_save_full_path = videos_save_path

    each_video_full_path = videos_src_path

    cap = cv2.VideoCapture(each_video_full_path)
    print(each_video_full_path)
    frame_count = 1
    success = True
    while (success):
        success, frame = cap.read()
        if success == True and frame_count % 30 == 0: # edit the frame interval here, 60 fps
        # if success == True:  # or uncomment it to save all frames
            cv2.imwrite(each_video_save_full_path + "%08d.jpg" % frame_count, frame)
        frame_count += 1



def select_frames(input_folder, output_folder, list1):
    files = sorted(
        [f for f in os.listdir(input_folder) if f.endswith('.jpg')],
        key=lambda x: int(x.split('.')[0])
    )
    
    selected_files = files[::2]
    
    if len(selected_files) < len(list1):
        selected_files += [selected_files[-1]] * (len(list1) - len(selected_files))
    # print(len(selected_files))
    # print(len(list1))
    # if len(selected_files) != len(list1):
    #     raise ValueError("The number of selected files does not match the number of names in the list.")
    
    if not os.path.exists(output_folder):
        os.makedirs(output_folder, exist_ok=True)
    
    for old_name, new_name in zip(selected_files, list1):
        src = os.path.join(input_folder, old_name)
        dst = os.path.join(output_folder, f"{new_name}.jpg")
        shutil.copy2(src, dst)



if __name__ == '__main__':
    process_json_critic_scores()
